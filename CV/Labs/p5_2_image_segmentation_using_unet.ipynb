{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"p5_2_image_segmentation_using_unet.ipynb","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/oxford_pets_image_segmentation.ipynb","timestamp":1606400484486}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2o1GwGEssa2j"},"source":["# Laboratory #5_2 : Image Segmentation using UNet\n","\n","At the end of this laboratory, you would get familiarized with\n","\n","*   Segmentation using UNet\n","*   Understand the evaluation metrics\n","*   Importance of data annotations\n","\n","**Remember this is a graded exercise.**\n","\n","*   For every plot, make sure you provide appropriate titles, axis labels, legends, wherever applicable.\n","*   Create reusable functions where ever possible, so that the code could be reused at different places.\n","*   Add sufficient comments and explanations wherever necessary.\n","*   **Once you have the code completed, use GPU to train model faster.**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1zBfZflkZ4sN"},"source":["# Loading dataset\n","\n","*   We will use the [Tray Food Segmentation dataset](https://www.kaggle.com/thezaza102/tray-food-segmentation) for this laboratory.\n","*   You have two options to get the dataset into your notebook\n","    *   The dataset is already available in campus virtual. Upload the dataset to your drive before starting the exercise.\n","    *   You can use the kaggle APIs to get the dataset into the notebook directly (Advanced option)."]},{"cell_type":"code","metadata":{"id":"10OQh5XmczVi"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUsFmBQM991i"},"source":["# Constants\n","\n","*   Change the path of the directories according to your drive location."]},{"cell_type":"code","metadata":{"id":"Lc36Vcmce_DR"},"source":["import os\n","\n","root_path = r''\n","\n","train_img = os.path.join(root_path, 'XTrain')\n","train_mask = os.path.join(root_path, 'yTrain')\n","test_img = os.path.join(root_path, 'XTest')\n","test_mask = os.path.join(root_path, 'yTest')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JubjGi8QfeSK"},"source":["*   Define the constants needed for training the model"]},{"cell_type":"code","metadata":{"id":"q2XDuHw3eeK9"},"source":["img_size = (160, 160)\n","num_classes = 43  # fixed for this dataset\n","batch_size = 32\n","epochs = 25"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvDvXEngeeK9"},"source":["# Paths to Input Images and Segmentation Masks\n","\n","*   We prepare the list of images and masks for both the train and test set"]},{"cell_type":"code","metadata":{"id":"NXgx3a1W951V"},"source":["import os\n","\n","train_img_paths = sorted(\n","    [\n","        os.path.join(train_img, fname)\n","        for fname in os.listdir(train_img)\n","        if fname.endswith(\".jpg\") or fname.endswith(\".JPG\")\n","    ]\n",")\n","\n","train_mask_paths = sorted(\n","    [\n","        os.path.join(train_mask, fname)\n","        for fname in os.listdir(train_mask)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","test_img_paths = sorted(\n","    [\n","        os.path.join(test_img, fname)\n","        for fname in os.listdir(test_img)\n","        if fname.endswith(\".jpg\") or fname.endswith(\".JPG\")\n","    ]\n",")\n","\n","test_mask_paths = sorted(\n","    [\n","        os.path.join(test_mask, fname)\n","        for fname in os.listdir(test_mask)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","print(\"Number of train samples:\", len(train_img_paths))\n","print(\"Number of train masks:\", len(train_mask_paths))\n","\n","print(\"Number of test samples:\", len(test_img_paths))\n","print(\"Number of test masks:\", len(test_mask_paths))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HnbWBareeK9"},"source":["# Visualizing input image and corresponding segmentation mask\n","\n","*   Visualize one train image and the corresponding segmentation mask in a matplotlib subplot\n","*   Also, show the mask overlayed on the original image."]},{"cell_type":"code","metadata":{"id":"bGZzIiceeeK9"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjmtLpA2eeK9"},"source":["# Image Generator\n","\n","*   Create an image generator class (similar to the tutorial) to iterate over the images and return a tuple corresponding to the batch number.\n","*   The generator should inherit from the Sequence class.\n","*   It should have \\_\\_init\\_\\_(), \\_\\_len\\_\\_() and \\_\\_getitem\\_\\_() methods.\n","*   The batch size, image size, image paths and mask paths should be initialized using the \\_\\_init\\_\\_ method."]},{"cell_type":"code","metadata":{"id":"7JlNR7yteeK9"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSj3e3TieeK9"},"source":["# U-Net Xception-style model\n","\n","*   We will use the same model architecture that we followed in the tutorial for this exercise"]},{"cell_type":"code","metadata":{"id":"PAmTmywreeK9"},"source":["from tensorflow.keras import layers\n","\n","\n","def get_model(img_size, num_classes):\n","    inputs = keras.Input(shape=img_size + (3,))\n","\n","    ### [First half of the network: downsampling inputs] ###\n","\n","    # Entry block\n","    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    previous_block_activation = x  # Set aside residual\n","\n","    # Blocks 1, 2, 3 are identical apart from the feature depth.\n","    for filters in [64, 128, 256]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n","\n","        # Project residual\n","        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n","            previous_block_activation\n","        )\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    ### [Second half of the network: upsampling inputs] ###\n","\n","    for filters in [256, 128, 64, 32]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.UpSampling2D(2)(x)\n","\n","        # Project residual\n","        residual = layers.UpSampling2D(2)(previous_block_activation)\n","        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    # Add a per-pixel classification layer\n","    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n","\n","    # Define the model\n","    model = keras.Model(inputs, outputs)\n","    return model\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zyaMy6rHJ24T"},"source":["# Free up RAM in case the model definition cells were run multiple times\n","\n","import keras\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMwalq3AJzV6"},"source":["# Build model\n","\n","model = get_model(img_size, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mudxhXaXmQRN"},"source":["*   Print the summary of the model"]},{"cell_type":"code","metadata":{"id":"tOfBJjCUEAkR"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qewnJAkaeeK9"},"source":["# Instantiate Image Generators for each split\n","\n","*   For this experiment, we will use the test split as the validation generator"]},{"cell_type":"code","metadata":{"id":"c43PzegLeeK9"},"source":["import random\n","\n","random.Random(1337).shuffle(train_img_paths)\n","random.Random(1337).shuffle(train_mask_paths)\n","\n","# Instantiate data Sequences for each split\n","train_gen = ImageGeneator(batch_size, img_size, train_img_paths, train_mask_paths)\n","\n","val_gen = ImageGeneator(batch_size, img_size, test_img_paths, test_mask_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFfhTs_zeeK9"},"source":["# Train Model\n","\n","*   Configure the model for training\n","    *   Use 'Sparse Categorical Crossentropy' loss for training\n","    *   Use 'rmsprop' optimizer"]},{"cell_type":"code","metadata":{"id":"N-VssMYUeeK9"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PE8nD0TFrs28"},"source":["*   Instantiate a ModelCheckpoint callback to save only the best model"]},{"cell_type":"code","metadata":{"id":"J1UssfCVrobq"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5EKaY_2sEbP"},"source":["*   Train the model using fit method"]},{"cell_type":"code","metadata":{"id":"GsPonCwYrq2R"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YiaxFw19eeK9"},"source":["# Visualize predictions\n","\n","*   Generate predictions for all images in the validation set"]},{"cell_type":"code","metadata":{"id":"jSUUNqc26jRh"},"source":["batch_size = 1  # we define batch_size as 1 for inferencing\n","\n","val_gen = ImageGeneator(batch_size, img_size, test_img_paths, test_mask_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtq3qvE5xKhF"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hewMZJJr7r-F"},"source":["*   The mask is determined by the maximum value along the final axis. For all the predictions get the value of the predicted mask."]},{"cell_type":"code","metadata":{"id":"_VOcSmAG6wxi"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"463_MLub-A3_"},"source":["*   Read the test mask images to create the ground truth"]},{"cell_type":"code","metadata":{"id":"apD0cabF9gZK"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ximHk6P-39Q"},"source":["*   For each validation image, find the jaccard similarity score"]},{"cell_type":"code","metadata":{"id":"i6eBNwUi-u8d"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7BWMpm4iFQ1"},"source":["*   What do you understand from the 'average' parameter of the Jaccard similarity score function?\n","*   What are the different values of 'average' parameter? \n","*   Which parameter is used for what type of problem?"]},{"cell_type":"markdown","metadata":{"id":"7Lyt2KtJiEM_"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"2YM0os9UhYAJ"},"source":["*   Randomly display one image, groundtruth mask, overlayed groundtruth mask, prediction mask, overlayed prediction mask using matplotlib "]},{"cell_type":"code","metadata":{"id":"1CXTqOwHeeK9"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRIZChJchs-X"},"source":["*   What are other evaluation metrics to compare the performance of segmentation?"]},{"cell_type":"markdown","metadata":{"id":"Ri9kU3wa3Rei"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"P3cxHHrxjf8X"},"source":["# Testing Model in the wild\n","\n","*   Download 5 images from the internet similar to the dataset used.\n","*   Use the trained model to segment the images"]},{"cell_type":"code","metadata":{"id":"fjCGUS9hhrE3"},"source":["# solution\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyWdN9Foksy7"},"source":["*   How accurate are the prediction masks?\n","*   What happens to the unlabelled classes?"]},{"cell_type":"markdown","metadata":{"id":"WAO9wTaMlFq4"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"iqTdiCOcltn0"},"source":["*   What are the challenges when you use a trained model in the wild?\n","*   How can you develop a model to be used in real world?"]},{"cell_type":"markdown","metadata":{"id":"RRvzK4zMl1wh"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","source":["*   What is the difference between instance segmentation and semantic segmentation?"],"metadata":{"id":"K2MMOQnpKSlv"}},{"cell_type":"markdown","metadata":{"id":"aE_xanr6M3j-"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","source":["*   What is the difference between encoder and decoder? Should they be of the same type of neural network?"],"metadata":{"id":"LVKtKkrmK4yv"}},{"cell_type":"markdown","metadata":{"id":"rggzKCVNM3yj"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","source":["*   What is upsampling?"],"metadata":{"id":"1X3I4C0oLGsH"}},{"cell_type":"markdown","metadata":{"id":"m_3yxyTDM4Ku"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","source":["*   What are feature channels in UNet? Why is this helpful in this architecture?"],"metadata":{"id":"FppsTWBWMbxm"}},{"cell_type":"markdown","metadata":{"id":"guWhsJDNM4ei"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","source":["*   What are some applications of U-Net?"],"metadata":{"id":"A3geOD_UMsEd"}},{"cell_type":"markdown","metadata":{"id":"DqU0VyBMM431"},"source":["**Solution**\n","\n","*(Double-click or enter to edit)*\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"NnN_t5Me7N5O"},"source":["\n","---\n","\n","## **End of P5_2: Image Segmentation using UNet**\n","Deadline for P5_2 submission in CampusVirtual is: **Monday, the 3rd of January, 2022**"]}]}