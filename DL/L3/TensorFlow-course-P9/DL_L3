{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_L3","provenance":[],"collapsed_sections":["2Pu9aGDgE-ER","bUafzEZtkbLX"],"authorship_tag":"ABX9TyPF1kT7kdTlnNLSwbmmkpPi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnAwrTChCFva","executionInfo":{"status":"ok","timestamp":1654730354154,"user_tz":-120,"elapsed":5195,"user":{"displayName":"Ramon Mateo Navarro","userId":"07442703126175469251"}},"outputId":"c4b42b4f-6352-48e5-ac3c-fd292b5eb5d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")\n"]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/DL/L3/TensorFlow-course-P9/* ."],"metadata":{"id":"alG8-yIDEfDU","executionInfo":{"status":"ok","timestamp":1654730355198,"user_tz":-120,"elapsed":1050,"user":{"displayName":"Ramon Mateo Navarro","userId":"07442703126175469251"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n"],"metadata":{"id":"I74Eov1wllUS","executionInfo":{"status":"ok","timestamp":1654673701424,"user_tz":-120,"elapsed":3155,"user":{"displayName":"Ramon Mateo Navarro","userId":"07442703126175469251"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"81896f9e-92c8-4e02-cac7-02ae80d70e29"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}]},{"cell_type":"markdown","source":["## P1"],"metadata":{"id":"2Pu9aGDgE-ER"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","tf.disable_v2_behavior()\n","lr_list = [0.01, 0.001, 0.0001, 0.00001]\n","\n","\n","\n","# optimizer\n","for j in range(len(lr_list)):\n","  # Model parameters\n","  W = tf.Variable([.3], dtype=tf.float32)\n","  b = tf.Variable([-.3], dtype=tf.float32)\n","  # Model input and output\n","  x = tf.placeholder(tf.float32)\n","  linear_model = W * x + b\n","  y = tf.placeholder(tf.float32)\n","\n","# loss\n","  loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n","  optimizer = tf.train.AdamOptimizer(lr_list[j])\n","  train = optimizer.minimize(loss)\n","  print(\"Current learning rate:\", lr_list[j])\n","  # training data\n","  x_train = [1, 2, 3, 4]\n","  y_train = [0, -1, -2, -3]\n","  # training loop\n","  init = tf.global_variables_initializer()\n","  sess = tf.Session()\n","  sess.run(init) # reset values to wrong\n","  \n","  curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n","  print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\n","  list_lost = [curr_loss]\n","\n","  for i in range(1000):\n","    sess.run(train, {x: x_train, y: y_train})\n","    curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n","    print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\n","    list_lost.append(curr_loss)\n","  \n","  fig = sns.lineplot(x=[i for i in range(1001)], y=list_lost)\n","  fig = fig.get_figure()\n","  fig.savefig(\"Adam_descent_lineplot_\" + str(lr_list[j]) + \".png\")\n","  plt.close(fig)"],"metadata":{"id":"xj4ZrRzxEph1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## P2"],"metadata":{"id":"bUafzEZtkbLX"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","import read_inputs\n","import numpy as N\n","\n","\n","#read data from file\n","data_input = read_inputs.load_data_mnist('MNIST_data/mnist.pkl.gz')\n","#FYI data = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n","data = data_input[0]\n","#print ( N.shape(data[0][0])[0] )\n","#print ( N.shape(data[0][1])[0] )\n","\n","#data layout changes since output should an array of 10 with probabilities\n","real_output = N.zeros( (N.shape(data[0][1])[0] , 10), dtype=N.float )\n","for i in range ( N.shape(data[0][1])[0] ):\n","  real_output[i][data[0][1][i]] = 1.0  \n","\n","#data layout changes since output should an array of 10 with probabilities\n","real_check = N.zeros( (N.shape(data[2][1])[0] , 10), dtype=N.float )\n","for i in range ( N.shape(data[2][1])[0] ):\n","  real_check[i][data[2][1][i]] = 1.0\n","\n","lr_list = [0.01, 0.001, 0.0001, 0.00001]\n","for k in range(len(lr_list)):\n","  #set up the computation. Definition of the variables.\n","  x = tf.placeholder(tf.float32, [None, 784])\n","  W = tf.Variable(tf.zeros([784, 10]))\n","  b = tf.Variable(tf.zeros([10]))\n","  y = tf.nn.softmax(tf.matmul(x, W) + b)\n","  y_ = tf.placeholder(tf.float32, [None, 10])\n","\n","  cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n","\n","  train_step = tf.train.AdadeltaOptimizer(lr_list[k]).minimize(cross_entropy)\n","\n","  sess = tf.InteractiveSession()\n","  tf.global_variables_initializer().run()\n","  cross_list, accuracy_list = [], []\n","  #TRAINING PHASE\n","  print(\"TRAINING\")\n","\n","  for i in range(500):\n","    batch_xs = data[0][0][100*i:100*i+100]\n","    batch_ys = real_output[100*i:100*i+100]\n","    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n","    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    cross_list.append(sess.run(cross_entropy, feed_dict={x: data[2][0], y_: real_check}))\n","    accuracy_list.append(sess.run(accuracy, feed_dict={x: data[2][0], y_: real_check}))\n","\n","  fig = sns.lineplot(x=[i for i in range(500)], y=cross_list)\n","  fig = fig.get_figure()\n","  fig.savefig(\"Adadelta_lineplot_cross\" + str(lr_list[k]) + \".png\")\n","  plt.close(fig)\n","  fig = sns.lineplot(x=[i for i in range(500)], y=accuracy_list)\n","  fig = fig.get_figure()\n","  fig.savefig(\"Adadelta_lineplot_acc\" + str(lr_list[k]) + \".png\")\n","  plt.close(fig)\n","\"\"\"\n","  #CHECKING THE ERROR\n","  print(\"ERROR CHECK\")\n","\n","  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","  print(sess.run(accuracy, feed_dict={x: data[2][0], y_: real_check}))\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"yA-ieUlvkaEv","executionInfo":{"status":"ok","timestamp":1654610924977,"user_tz":-120,"elapsed":2866415,"user":{"displayName":"Ramon Mateo Navarro","userId":"07442703126175469251"}},"outputId":"a52b5cde-2779-41f1-cb5b-4fddec3c4621"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n  #CHECKING THE ERROR\\n  print(\"ERROR CHECK\")\\n\\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\\n  print(sess.run(accuracy, feed_dict={x: data[2][0], y_: real_check}))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## P3"],"metadata":{"id":"O5h0WOwp9UJv"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","import read_inputs\n","import numpy as N\n","import time\n","import random\n","#print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","\n","#read data from file\n","data_input = read_inputs.load_data_mnist('MNIST_data/mnist.pkl.gz')\n","#FYI data = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n","data = data_input[0]\n","#print ( N.shape(data[0][0])[0] )\n","#print ( N.shape(data[0][1])[0] )\n","\n","N_GPU = 4\n","\n","#data layout changes since output should an array of 10 with probabilities\n","real_output = N.zeros( (N.shape(data[0][1])[0] , 10), dtype=N.float )\n","for i in range ( N.shape(data[0][1])[0] ):\n","  real_output[i][data[0][1][i]] = 1.0  \n","\n","\n","#data layout changes since output should an array of 10 with probabilities\n","real_check = N.zeros( (N.shape(data[2][1])[0] , 10), dtype=N.float )\n","for i in range ( N.shape(data[2][1])[0] ):\n","  real_check[i][data[2][1][i]] = 1.0\n","\n","\n","#set up the computation. Definition of the variables.\n","x = tf.placeholder(tf.float32, [None, 784])\n","W = tf.Variable(tf.zeros([784, 10]))\n","y_ = tf.placeholder(tf.float32, [None, 10])\n","\n","\n","\n","#declare weights and biases\n","def weight_variable(shape):\n","  initial = tf.truncated_normal(shape, stddev=0.1)\n","  return tf.Variable(initial)\n","\n","def bias_variable(shape):\n","  initial = tf.constant(0.1, shape=shape)\n","  return tf.Variable(initial)\n","\n","\n","#convolution and pooling\n","def conv2d(x, W):\n","  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","\n","def max_pool_2x2(x):\n","  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n","                        strides=[1, 2, 2, 1], padding='SAME')\n","\n","\n","\n","\n","#First convolutional layer: 32 features per each 5x5 patch\n","W_conv1 = weight_variable([5, 5, 1, 32])\n","b_conv1 = bias_variable([32])\n","\n","\n","#Reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height.\n","#28x28 = 784\n","#The final dimension corresponding to the number of color channels.\n","x_image = tf.reshape(x, [-1, 28, 28, 1])\n","\n","\n","#We convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. \n","#The max_pool_2x2 method will reduce the image size to 14x14.\n","\n","h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n","h_pool1 = max_pool_2x2(h_conv1)\n","\n","\n","\n","#Second convolutional layer: 64 features for each 5x5 patch.\n","W_conv2 = weight_variable([5, 5, 32, 64])\n","b_conv2 = bias_variable([64])\n","\n","h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n","h_pool2 = max_pool_2x2(h_conv2)\n","\n","\n","#Densely connected layer: Processes the 64 7x7 images with 1024 neurons\n","#Reshape the tensor from the pooling layer into a batch of vectors, \n","#multiply by a weight matrix, add a bias, and apply a ReLU.\n","W_fc1 = weight_variable([7 * 7 * 64, 1024])\n","b_fc1 = bias_variable([1024])\n","\n","h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n","h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n","\n","#drop_out\n","keep_prob = tf.placeholder(tf.float32)\n","h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n","\n","\n","#Readout Layer\n","W_fc2 = weight_variable([1024, 10])\n","b_fc2 = bias_variable([10])\n","\n","y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n","\n","#Per_image_crossentropy\n","cross_entropy_local = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\n","\n","#Crossentropy\n","cross_entropy = tf.reduce_mean(cross_entropy_local)\n","#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n","\n","train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n","\n","correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n","\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","data_train = data[0][0]\n","\n","with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n","  sess.run(tf.global_variables_initializer())\n","  #TRAIN \n","  print(\"TRAINING\")\n","\n","  start_time = time.time()\n","  for _ in range(5):\n","    for i in range(3125):\n","\n","      #until 1000 96,35%\n","      batch_ini = 16*i\n","      batch_end = 16*i+16 # 50 = 96.3 25=95.7\n","\n","\n","      for j in range(N_GPU):\n","        with tf.device('/gpu:%d' %j):\n","          batch_xs = data_train[batch_ini:batch_end]\n","          batch_ys = real_output[batch_ini:batch_end]\n","\n","\n","      if i % 100 == 0:\n","        train_accuracy = accuracy.eval(feed_dict={\n","            x: batch_xs, y_: batch_ys, keep_prob: 1.0})\n","        print('step %d, training accuracy %g Batch [%d,%d]' % (i, train_accuracy, batch_ini, batch_end))\n","\n","      train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.75})\n","    c = list(zip(data_train, real_output))\n","    random.shuffle(c)\n","    data_train , real_output = zip(*c)\n","\n","\n","  print(\"Training Time: %.3f seconds\" % (time.time() - start_time))\n","  #TEST\n","  print(\"TESTING\")\n","\n","  train_accuracy = accuracy.eval(feed_dict={x: data[2][0], y_: real_check, keep_prob: 1.0})\n","  print('test accuracy %.3f' %(train_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPqJ_bzM9W1O","executionInfo":{"status":"ok","timestamp":1654682636624,"user_tz":-120,"elapsed":150959,"user":{"displayName":"Ramon Mateo Navarro","userId":"07442703126175469251"}},"outputId":"5644b506-591f-4bd2-83b5-4b5b13bf9e31"},"execution_count":27,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["Device mapping: no known devices.\n","TRAINING\n","step 0, training accuracy 0.0625 Batch [0,16]\n","step 100, training accuracy 0.75 Batch [1600,1616]\n","step 200, training accuracy 0.9375 Batch [3200,3216]\n","step 300, training accuracy 0.9375 Batch [4800,4816]\n","step 400, training accuracy 1 Batch [6400,6416]\n","step 500, training accuracy 0.875 Batch [8000,8016]\n","step 600, training accuracy 0.875 Batch [9600,9616]\n","step 700, training accuracy 0.8125 Batch [11200,11216]\n","step 800, training accuracy 0.9375 Batch [12800,12816]\n","step 900, training accuracy 0.9375 Batch [14400,14416]\n","step 1000, training accuracy 0.9375 Batch [16000,16016]\n","step 1100, training accuracy 0.9375 Batch [17600,17616]\n","step 1200, training accuracy 0.9375 Batch [19200,19216]\n","step 1300, training accuracy 1 Batch [20800,20816]\n","step 1400, training accuracy 0.875 Batch [22400,22416]\n","step 1500, training accuracy 1 Batch [24000,24016]\n","step 1600, training accuracy 1 Batch [25600,25616]\n","step 1700, training accuracy 1 Batch [27200,27216]\n","step 1800, training accuracy 0.9375 Batch [28800,28816]\n","step 1900, training accuracy 1 Batch [30400,30416]\n","step 2000, training accuracy 0.9375 Batch [32000,32016]\n","step 2100, training accuracy 0.9375 Batch [33600,33616]\n","step 2200, training accuracy 0.9375 Batch [35200,35216]\n","step 2300, training accuracy 1 Batch [36800,36816]\n","step 2400, training accuracy 0.9375 Batch [38400,38416]\n","step 2500, training accuracy 1 Batch [40000,40016]\n","step 2600, training accuracy 0.9375 Batch [41600,41616]\n","step 2700, training accuracy 0.9375 Batch [43200,43216]\n","step 2800, training accuracy 1 Batch [44800,44816]\n","step 2900, training accuracy 0.9375 Batch [46400,46416]\n","step 3000, training accuracy 1 Batch [48000,48016]\n","step 3100, training accuracy 1 Batch [49600,49616]\n","step 0, training accuracy 1 Batch [0,16]\n","step 100, training accuracy 1 Batch [1600,1616]\n","step 200, training accuracy 1 Batch [3200,3216]\n","step 300, training accuracy 1 Batch [4800,4816]\n","step 400, training accuracy 1 Batch [6400,6416]\n","step 500, training accuracy 1 Batch [8000,8016]\n","step 600, training accuracy 1 Batch [9600,9616]\n","step 700, training accuracy 1 Batch [11200,11216]\n","step 800, training accuracy 1 Batch [12800,12816]\n","step 900, training accuracy 0.9375 Batch [14400,14416]\n","step 1000, training accuracy 1 Batch [16000,16016]\n","step 1100, training accuracy 0.9375 Batch [17600,17616]\n","step 1200, training accuracy 1 Batch [19200,19216]\n","step 1300, training accuracy 1 Batch [20800,20816]\n","step 1400, training accuracy 1 Batch [22400,22416]\n","step 1500, training accuracy 1 Batch [24000,24016]\n","step 1600, training accuracy 0.875 Batch [25600,25616]\n","step 1700, training accuracy 1 Batch [27200,27216]\n","step 1800, training accuracy 0.9375 Batch [28800,28816]\n","step 1900, training accuracy 1 Batch [30400,30416]\n","step 2000, training accuracy 0.9375 Batch [32000,32016]\n","step 2100, training accuracy 1 Batch [33600,33616]\n","step 2200, training accuracy 1 Batch [35200,35216]\n","step 2300, training accuracy 0.875 Batch [36800,36816]\n","step 2400, training accuracy 0.9375 Batch [38400,38416]\n","step 2500, training accuracy 1 Batch [40000,40016]\n","step 2600, training accuracy 1 Batch [41600,41616]\n","step 2700, training accuracy 1 Batch [43200,43216]\n","step 2800, training accuracy 1 Batch [44800,44816]\n","step 2900, training accuracy 1 Batch [46400,46416]\n","step 3000, training accuracy 1 Batch [48000,48016]\n","step 3100, training accuracy 1 Batch [49600,49616]\n","step 0, training accuracy 1 Batch [0,16]\n","step 100, training accuracy 1 Batch [1600,1616]\n","step 200, training accuracy 1 Batch [3200,3216]\n","step 300, training accuracy 0.9375 Batch [4800,4816]\n","step 400, training accuracy 1 Batch [6400,6416]\n","step 500, training accuracy 1 Batch [8000,8016]\n","step 600, training accuracy 1 Batch [9600,9616]\n","step 700, training accuracy 1 Batch [11200,11216]\n","step 800, training accuracy 0.875 Batch [12800,12816]\n","step 900, training accuracy 1 Batch [14400,14416]\n","step 1000, training accuracy 1 Batch [16000,16016]\n","step 1100, training accuracy 1 Batch [17600,17616]\n","step 1200, training accuracy 1 Batch [19200,19216]\n","step 1300, training accuracy 0.9375 Batch [20800,20816]\n","step 1400, training accuracy 1 Batch [22400,22416]\n","step 1500, training accuracy 1 Batch [24000,24016]\n","step 1600, training accuracy 1 Batch [25600,25616]\n","step 1700, training accuracy 1 Batch [27200,27216]\n","step 1800, training accuracy 1 Batch [28800,28816]\n","step 1900, training accuracy 0.8125 Batch [30400,30416]\n","step 2000, training accuracy 0.9375 Batch [32000,32016]\n","step 2100, training accuracy 1 Batch [33600,33616]\n","step 2200, training accuracy 1 Batch [35200,35216]\n","step 2300, training accuracy 1 Batch [36800,36816]\n","step 2400, training accuracy 1 Batch [38400,38416]\n","step 2500, training accuracy 1 Batch [40000,40016]\n","step 2600, training accuracy 1 Batch [41600,41616]\n","step 2700, training accuracy 1 Batch [43200,43216]\n","step 2800, training accuracy 1 Batch [44800,44816]\n","step 2900, training accuracy 0.9375 Batch [46400,46416]\n","step 3000, training accuracy 0.9375 Batch [48000,48016]\n","step 3100, training accuracy 1 Batch [49600,49616]\n","step 0, training accuracy 1 Batch [0,16]\n","step 100, training accuracy 1 Batch [1600,1616]\n","step 200, training accuracy 0.9375 Batch [3200,3216]\n","step 300, training accuracy 1 Batch [4800,4816]\n","step 400, training accuracy 1 Batch [6400,6416]\n","step 500, training accuracy 1 Batch [8000,8016]\n","step 600, training accuracy 1 Batch [9600,9616]\n","step 700, training accuracy 1 Batch [11200,11216]\n","step 800, training accuracy 1 Batch [12800,12816]\n","step 900, training accuracy 1 Batch [14400,14416]\n","step 1000, training accuracy 1 Batch [16000,16016]\n","step 1100, training accuracy 1 Batch [17600,17616]\n","step 1200, training accuracy 1 Batch [19200,19216]\n","step 1300, training accuracy 1 Batch [20800,20816]\n","step 1400, training accuracy 1 Batch [22400,22416]\n","step 1500, training accuracy 1 Batch [24000,24016]\n","step 1600, training accuracy 1 Batch [25600,25616]\n","step 1700, training accuracy 0.9375 Batch [27200,27216]\n","step 1800, training accuracy 1 Batch [28800,28816]\n","step 1900, training accuracy 1 Batch [30400,30416]\n","step 2000, training accuracy 1 Batch [32000,32016]\n","step 2100, training accuracy 1 Batch [33600,33616]\n","step 2200, training accuracy 0.9375 Batch [35200,35216]\n","step 2300, training accuracy 1 Batch [36800,36816]\n","step 2400, training accuracy 1 Batch [38400,38416]\n","step 2500, training accuracy 1 Batch [40000,40016]\n","step 2600, training accuracy 1 Batch [41600,41616]\n","step 2700, training accuracy 1 Batch [43200,43216]\n","step 2800, training accuracy 1 Batch [44800,44816]\n","step 2900, training accuracy 1 Batch [46400,46416]\n","step 3000, training accuracy 1 Batch [48000,48016]\n","step 3100, training accuracy 0.9375 Batch [49600,49616]\n","step 0, training accuracy 1 Batch [0,16]\n","step 100, training accuracy 1 Batch [1600,1616]\n","step 200, training accuracy 1 Batch [3200,3216]\n","step 300, training accuracy 1 Batch [4800,4816]\n","step 400, training accuracy 1 Batch [6400,6416]\n","step 500, training accuracy 0.9375 Batch [8000,8016]\n","step 600, training accuracy 1 Batch [9600,9616]\n","step 700, training accuracy 1 Batch [11200,11216]\n","step 800, training accuracy 1 Batch [12800,12816]\n","step 900, training accuracy 1 Batch [14400,14416]\n","step 1000, training accuracy 1 Batch [16000,16016]\n","step 1100, training accuracy 1 Batch [17600,17616]\n","step 1200, training accuracy 0.9375 Batch [19200,19216]\n","step 1300, training accuracy 1 Batch [20800,20816]\n","step 1400, training accuracy 1 Batch [22400,22416]\n","step 1500, training accuracy 1 Batch [24000,24016]\n","step 1600, training accuracy 1 Batch [25600,25616]\n","step 1700, training accuracy 1 Batch [27200,27216]\n","step 1800, training accuracy 1 Batch [28800,28816]\n","step 1900, training accuracy 1 Batch [30400,30416]\n","step 2000, training accuracy 1 Batch [32000,32016]\n","step 2100, training accuracy 1 Batch [33600,33616]\n","step 2200, training accuracy 1 Batch [35200,35216]\n","step 2300, training accuracy 1 Batch [36800,36816]\n","step 2400, training accuracy 1 Batch [38400,38416]\n","step 2500, training accuracy 1 Batch [40000,40016]\n","step 2600, training accuracy 1 Batch [41600,41616]\n","step 2700, training accuracy 1 Batch [43200,43216]\n","step 2800, training accuracy 1 Batch [44800,44816]\n","step 2900, training accuracy 1 Batch [46400,46416]\n","step 3000, training accuracy 1 Batch [48000,48016]\n","step 3100, training accuracy 1 Batch [49600,49616]\n","Training Time: 1043.783 seconds\n","TESTING\n","test accuracy 0.990\n"]}]},{"cell_type":"code","source":["x = data[0][0]\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcAF7aYsagl9","executionInfo":{"status":"ok","timestamp":1654677841163,"user_tz":-120,"elapsed":319,"user":{"displayName":"Ramon Mateo Navarro","userId":"07442703126175469251"}},"outputId":"84d2abe2-2e54-4b53-f58b-9e0af2246719"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 784)"]},"metadata":{},"execution_count":23}]}]}