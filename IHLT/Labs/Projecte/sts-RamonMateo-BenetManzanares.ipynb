{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sts-RamonMateo-BenetManzanares.ipynb","provenance":[],"collapsed_sections":["YR5-Jjd0O19d"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZFNv4RfqKFTy"},"source":["#Project : Semantic Textual Similarity\n","**Authors:**\n","\n","* Ramón Mateo Navarro\n","* Benet Manzanares Salor"]},{"cell_type":"markdown","metadata":{"id":"t_XeL6yCKayN"},"source":["##Installation ans imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26kXPvNJKDDz","executionInfo":{"status":"ok","timestamp":1638981925741,"user_tz":-60,"elapsed":1355,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"2b7df013-b120-4b7c-fb78-c4ed52d568c9"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import re\n","from argparse import Namespace\n","from functools import partial\n","from itertools import chain\n","\n","from google.colab import drive\n","\n","import spacy\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","\n","from sklearn import linear_model\n","from sklearn.neural_network import MLPRegressor\n","from scipy.stats import pearsonr"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"yNLVfQN5JqHt"},"source":["## Settings \n","\n"]},{"cell_type":"code","metadata":{"id":"wH-xNKfiLqxv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638981929444,"user_tz":-60,"elapsed":3704,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"dc0849ee-4664-4a04-e4df-ccc4e9677bf4"},"source":["settings = Namespace()\n","\n","# Paths\n","settings.mount_path = \"/content/drive/\"\n","drive.mount(settings.mount_path, force_remount=True)\n","settings.drive_path = os.path.join(settings.mount_path, \"MyDrive\")\n","settings.project_folder = \"TO DO: Teacher PATH\" #@param [\"Benet_MAI/S1/IHLT/IHLT_Labs/Projecte\", \"MAI/IHLT/IHTL_Labs/Projecte\", \"TO DO: Teacher PATH\"]\n","settings.project_path = os.path.join(settings.drive_path, settings.project_folder)\n","settings.data_path = os.path.join(settings.project_path, \"Data\")\n","settings.train_data_path = os.path.join(settings.data_path, \"train\")\n","settings.test_data_path = os.path.join(settings.data_path, \"test-gold\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"Q1CAzxrvMm9r"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"WoZyxTPQ4QSt"},"source":["############# Functions #############\n","def load_dataframe(input_filepath):\n","  \"\"\"\n","  Load dataframe for a given input file path\n","  @param input_filepath: path to the input file\n","  @return a Pandas' dataframe with the sentence pairs and Gold Standard as columns\n","  \"\"\"\n","  current_file_path = input_filepath\n","  try:\n","    # Read inputs\n","    data = []\n","    with open(input_filepath, 'r') as f:\n","      lines = f.read().splitlines()\n","      for line in lines:\n","        data.append(line.split(\"\\t\"))\n","    df = pd.DataFrame(data, columns = [0, 1])\n","    \n","    # Read Gold Standard\n","    current_file_path = re.sub(\"input\", \"gs\", input_filepath)\n","    df[\"gs\"] = pd.read_csv(current_file_path, sep='\\t', header=None)\n","\n","    # Normalize Gold Standard for more intuitive comparisions\n","    df[\"gs\"] = df[\"gs\"] / 5\n","\n","  except Exception as e:\n","    raise Exception(f\"ERROR while reading {current_file_path}:\\n\\t{e}\")\n","\n","  return df\n","\n","\n","def load_dataset(data_path):\n","  \"\"\"\n","  Load dataset for a given path\n","  @param data_path: path to the data folder\n","  @return a dataframe with the content of the corresponding files\n","  \"\"\"\n","  dataset = None\n","  filenames = os.listdir(data_path)\n","  for filename in filenames:\n","    if filename.startswith(\"STS.input\"):\n","      file_path = os.path.join(data_path, filename)\n","      try:\n","        df = load_dataframe(file_path)\n","        if dataset is None:\n","          dataset = df\n","        else:\n","          dataset = pd.concat([dataset, df], ignore_index=True)\n","      except Exception as e:\n","        print(e)\n","  \n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avx_m5Ir6s08"},"source":["############# Read datasets #############\n","train_dataset = load_dataset(settings.train_data_path)\n","test_dataset = load_dataset(settings.test_data_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q0EQZrVeeJfr"},"source":["## Common"]},{"cell_type":"markdown","metadata":{"id":"49dt08ysXgDF"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"5YV_EP4ccWrQ"},"source":["############# Auxiliar objects #############\n","nlp = spacy.load('en_core_web_sm')\n","special_characters_pattern = re.compile(r\"[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+\")\n","stop_words = stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtlphhaNXhgz"},"source":["############# Functions #############\n","def remove_special_chars(sentence):\n","  \"\"\"\n","  Remove special chars\n","  @param sentence: string to remove special characters from\n","  @return a string with special characters removed\n","  \"\"\"\n","  # Forming contractions  (Reference webs: https://www.thefreedictionary.com/Forming-Contractions.htm and https://www.gymglish.com/en/gymglish/english-grammar/forming-contractions-arent-cant-id-youre-etc)\n","  sentence = sentence.replace(\"n't\", \" not\")\n","  sentence = sentence.replace(\"'m\", \" am\")  \n","  sentence = sentence.replace(\"'re\", \" are\")\n","  sentence = sentence.replace(\"'ll\", \" will\")  \n","  sentence = sentence.replace(\"'ve\", \" have\")\n","\n","  # Special characters\n","  sentence = re.sub(special_characters_pattern, \" \", sentence)  \n","\n","  return sentence\n","\n","\n","def tokenize(sentence):\n","  \"\"\"\n","  Tokenize sentence\n","  @param sentence: sentence to be tokenized\n","  @return a list with all the SpaCy tokens\n","  \"\"\"\n","  \n","  return nlp.tokenizer(sentence)\n","\n","\n","def remove_stopwords(tokens):\n","  \"\"\"\n","  Remove stop words in a list of tokens.\n","  @param tokens: SpaCy tokens to delete stop words\n","  @return tokens without stopwords\n","  \"\"\"\n","  return list(filter(lambda x: x.text not in stop_words, tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eBtjwDfXsiIm"},"source":["### Lemmatization and synsets"]},{"cell_type":"code","metadata":{"id":"O6UZ--2LHqJo"},"source":["############# Auxiliar objects #############\n","wnl = WordNetLemmatizer()\n","PoS_to_WN_dict = {\n","    # NLTK to WordNet\n","    \"NN\": \"n\",\n","    \"NNS\": \"n\",\n","    \"NNP\": \"n\",\n","    \"NNPS\": \"n\",\n","    \"VB\": \"v\",\n","    \"VBD\": \"v\",\n","    \"VBG\": \"v\",\n","    \"VBN\": \"v\",\n","    \"VBP\": \"v\",\n","    \"VBZ\": \"v\",\n","    \"RB\": \"r\",\n","    \"RBR\": \"r\",\n","    \"RBS\": \"r\",\n","    \"JJ\": \"a\",\n","    \"JJR\": \"a\",\n","    \"JJS\": \"a\",\n","    # SpaCy to WordNet\n","    \"ADJ\": \"a\",\n","    \"ADV\": \"r\",\n","    \"AUX\": \"v\",\n","    \"NOUN\": \"n\",\n","    \"VERB\": \"v\",\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WB2Vv5ZGoX1N"},"source":["############# Functions #############\n","def lemmatize(p):\n","  \"\"\"\n","  @param p: a tuple of the form (word, tag)\n","  @return: the lemma of the word\n","  \"\"\"\n","  if p[1][0] in PoS_to_WN_dict.keys():\n","    return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n","  return p[0]\n","\n","\n","def get_lemmas(sentence):\n","  \"\"\" \n","  @param sentece: a sentence\n","  @return: a list of lemmas and tags\n","  \"\"\"\n","  words = nltk.word_tokenize(sentence)\n","  tags = nltk.pos_tag(words)\n","  lemmas = [lemmatize(pair) for pair in tags]\n","  return lemmas, tags\n","\n","\n","def get_synsets(lemmas, tags):\n","  \"\"\"\n","  Get synsets for a given list of lemmas \n","  @param lemmas: list with lemmas\n","  @param tags: list with tags\n","  @return: list with synsets\n","  \"\"\"\n","  synsets = []\n","  for i in range(len(lemmas)):\n","    lemma = lemmas[i]\n","    if(tags[i][1] in PoS_to_WN_dict.keys()):      \n","      synset = nltk.wsd.lesk(lemmas, lemma, PoS_to_WN_dict[tags[i][1]])\n","      if synset is not None:\n","        synsets.append(synset)\n","\n","  return synsets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YR5-Jjd0O19d"},"source":["### Lemmas information content"]},{"cell_type":"code","metadata":{"id":"LB0nTIXLO4sf"},"source":["############# Compute lemmas dictionary #############\n","sentences = list(train_dataset[0])\n","sentences += list(train_dataset[1])\n","lemmas_dict = {}\n","num_lemmas = 0\n","for sent in sentences:\n","  for token in tokenize(sent):\n","    lemma = token.lemma_.lower()\n","    lemmas_dict[lemma] = lemmas_dict.get(lemma, 0) + 1\n","    num_lemmas += 1\n","\n","############# Compute lemmas information content dictionary #############\n","lemmas_info_dict = {}\n","maximum_lemma_info = 0\n","for lemma, count in lemmas_dict.items():\n","  prob = count / num_lemmas\n","  lemmas_info_dict[lemma] = -np.log(prob)\n","  if lemmas_info_dict[lemma] > maximum_lemma_info:\n","    maximum_lemma_info = lemmas_info_dict[lemma]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhdjIvKwBHag"},"source":["### Distances"]},{"cell_type":"code","metadata":{"id":"GJQeYl77NbBM"},"source":["######### Custom set longitude function #########\n","def custom_length(set1:set, use_info_content=False):\n","  \"\"\"\n","  Computes length in set using lemmas information content if required.\n","  @param set1: set of elements\n","  @param use_info_content: If use the lemmas information content dictionary\n","  @return If set1 contains strings and use_info_content==True, the info content of the set. Otherwise, length of set1\n","  \"\"\"\n","  res = len(set1)\n","\n","  if res > 0:\n","    if use_info_content:\n","      res = 0\n","      for item in set1:\n","        if isinstance(item, str):\n","          res += lemmas_info_dict.get(item, maximum_lemma_info)\n","        else:\n","          res += maximum_lemma_info\n","  \n","  return res  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"piNMFeBaMouF"},"source":["######### Custom distance functions #########\n","def jaccard_similarity(set1:set, set2:set, use_info_content=False):\n","  \"\"\"\n","  Calculates the Jaccard similarity between two sets.\n","  @param set1: First set with sentence 1\n","  @param set2: Second set with sentence 2\n","  @param use_info_content: if True, information content is used to compute similarity\n","  @return: Jaccard similarity between the two sets\n","  \"\"\"\n","  sim = 0\n","  len_func = partial(custom_length, use_info_content=use_info_content)\n","\n","  denominator = (abs(len_func(set1.union(set2))))\n","  if denominator > 0:\n","    sim = abs(len_func(set1.intersection(set2))) / denominator\n","\n","  return sim\n","\n","\n","def dice_similarity(set1:set, set2:set, use_info_content=False):\n","  \"\"\"\n","  Computes Dice similarity between two sentences\n","  @param set1: set with first sentence\n","  @param set2: second sentence\n","  @param use_info_content: if True, information content is used to compute similarity\n","  @return: Dice similarity\n","  \"\"\"\n","  sim = 0\n","  len_func = partial(custom_length, use_info_content=use_info_content)\n","\n","  denominator =  abs(len_func(set1)) + abs(len_func(set2))\n","  if denominator > 0:\n","    sim = (2*abs(len_func(set1.intersection(set2)))) / denominator\n","\n","  return sim\n","\n","\n","def overlap_similarity(set1:set, set2:set, use_info_content=False):\n","  \"\"\"\n","  Computes Overlap similarity between two sentences\n","  @param set1: set with first sentence\n","  @param set2: set with second sentence\n","  @param use_info_content: if True, information content is used to compute similarity\n","  @return: Overlap similarity\n","  \"\"\"\n","  sim = 0\n","  len_func = partial(custom_length, use_info_content=use_info_content)\n","\n","  denominator = min(len_func(set1), len_func(set2))\n","  if denominator > 0:\n","    sim = (abs(len_func(set1.intersection(set2)))) / denominator\n","  return sim\n","\n","\n","def cosine_similarity(set1:set, set2:set, use_info_content=False):\n","  \"\"\"\n","  Computes Cosine similarity between two sentences\n","  @param set1: set with first sentence\n","  @param set2: set with second sentence\n","  @param use_info_content: if True, information content is used to compute similarity\n","  @return: Cosine similarity\n","  \"\"\"\n","  sim = 0\n","  len_func = partial(custom_length, use_info_content=use_info_content)\n","\n","  denominator = np.sqrt(abs(len_func(set1)*abs(len_func(set2))))\n","  if denominator > 0:\n","    sim = abs(len_func(set1.intersection(set2))) / denominator  \n","  return sim\n","\n","\n","set_sim_functions = {'jaccard': jaccard_similarity , 'overlap': overlap_similarity,\n","                      'dice': dice_similarity, 'cosine': cosine_similarity}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"joLs_mzlg4it"},"source":["### Auxiliar"]},{"cell_type":"code","metadata":{"id":"qTYd_b8Mg5uU"},"source":["# For n-grams similarity\n","def get_n_grams(sentence, n):\n","  \"\"\"\n","  Get the n-grams of a sentence using a sliding window of size n and stride 1\n","  @param sentence: sentence\n","  @param n: size of the sliding window\n","  @return: list of n-grams\n","  \"\"\"\n","  sentences = sentence.lower()\n","  n_grams_list = []\n","  ini = 0\n","  end = n\n","  while end < len(sentence):\n","    n_grams_list.append(sentence[ini:end])\n","    ini = end\n","    end += n\n","\n","  return n_grams_list\n","\n","\n","def replace_synonyms(sentence1, sentence2):\n","  \"\"\"\n","  @param sentence1: first sentence\n","  @param sentence2: second sentence\n","  @return two sentences with synonyms replaced\n","  \"\"\"\n","  for word in sentence1:\n","    # obtain the synonims of the word\n","    synonyms = wordnet.synsets(word)\n","    lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n","    for lemma in lemmas:\n","      if lemma in sentence2:\n","        # replace lemma in sentece2\n","        sentence2[sentence2.index(lemma)] = word\n","  return sentence2\n","\n","\n","# For lemmas synonyms similarity\n","def match_synonyms(sentence1, sentence2):\n","  \"\"\"\n","  Find synonyms in the two given sentences and replaces them  \n","  @param sentence1: first sentence\n","  @param sentence2: second sentence\n","  @return two sentences with synonyms matched and replaced\n","  \"\"\"\n","  a  = replace_synonyms(sentence1, sentence2)\n","  if a == sentence2:\n","    b = replace_synonyms(sentence2, sentence1)\n","  if sentence1 != sentence2:\n","    sentence1 = b\n","\n","  return sentence1, sentence2\n","\n","\n","# For regression methods\n","def get_sim_numpy_data(dataset, methods_dict):\n","  \"\"\"\n","  Gets the similarity of a given dataset\n","  @param dataset: Dataframe for obtaining all base similarity measures \n","  @return Numpy array with only base similarities results\n","  \"\"\"\n","  columns_names = list(methods_dict.keys())\n","  return dataset[columns_names].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fiaDnvPJtK6"},"source":["## Base similarities"]},{"cell_type":"code","metadata":{"id":"I3_kvXzQp8nc"},"source":["######### Choose the set similarity function to use by default #########\n","default_sim_function = set_sim_functions[\"jaccard\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Mdjl7lYBctt"},"source":["#### Lexical"]},{"cell_type":"code","metadata":{"id":"mRmarWd2DNfP"},"source":["def nltk_words_sim(sentences):\n","  \"\"\"\n","  Obtains the similarity between two sentences using nltk words technique\n","  @param senteces: tuple with two sentences\n","  @return: similarity between the two sentences using NLTK Words technique\n","  \"\"\"\n","  sentences = [remove_special_chars(s) for s in sentences]\n","  words = [nltk.word_tokenize(s) for s in sentences]\n","\n","  return default_sim_function(set(words[0]), set(words[1]))\n","\n","\n","def spacy_words_sim(sentences, use_preprocessing):\n","  \"\"\"\n","  Obtains the similarity between two sentences using spacy words technique\n","  @param senteces: tuple with two sentences\n","  @param use_preprocessing: if True, preprocessing is used\n","  @return: similarity between the two sentences using NLTK Words technique\n","  \"\"\"\n","  if use_preprocessing:\n","    sentences = [remove_special_chars(s) for s in sentences]\n","  tokens = [tokenize(s) for s in sentences]\n","  if use_preprocessing:\n","    tokens = [remove_stopwords(t) for t in tokens]\n","  \n","  words = [list(map(lambda x: x.text.lower(), t)) for t in tokens]\n","\n","  return default_sim_function(set(words[0]), set(words[1]))\n","\n","\n","def word_synonyms_sim(sentences):\n","  \"\"\"\n","  Computes the similarity between two sentences using word and synonyms technique\n","  @param senteces: tuple with two sentences\n","  @return: similarity between the two sentences using word and synonyms techniques\n","  \"\"\"\n","  sentences = [remove_special_chars(s) for s in sentences]\n","  tokens = [tokenize(s) for s in sentences]\n","  words = [list(map(lambda x: x.text.lower(), t)) for t in tokens]\n","\n","  words[0], words[1] = match_synonyms(words[0], words[1])\n","\n","  return default_sim_function(set(words[0]), set(words[1]))\n","\n","\n","def nltk_lemmas_sim(sentences):\n","  \"\"\"\n","  Computes the similarity between two sentences using nltk lemmas technique\n","  @param senteces: tuple with two sentences\n","  @return similarity between the two sentences using nltk lemmas technique\n","  \"\"\"\n","  sent1, sent2 = sentences\n","  lemmas1, tags1 = get_lemmas(sent1)\n","  lemmas2, tags2 = get_lemmas(sent2)\n","\n","  return default_sim_function(set(lemmas1), set(lemmas2), use_info_content=True)\n","\n","\n","def spacy_lemmas_sim(sentences):\n","  \"\"\"\n","  Obtains the similarity between two sentences using Spacy lemmas technique\n","  @param senteces: tuple with two sentences\n","  @return: similarity between the two sentences using Spacy lemmas technique\n","  \"\"\"\n","  sentences = [remove_special_chars(s) for s in sentences]\n","  tokens = [tokenize(s) for s in sentences]\n","  # Not stopwords removing\n","  \n","  lemmas = [list(map(lambda x: x.lemma_.lower(), t)) for t in tokens]\n","\n","  return default_sim_function(set(lemmas[0]), set(lemmas[1]), use_info_content=True)\n","\n","\n","def lemmas_synonyms_sim(sentences):\n","  \"\"\"\n","  Obtains the similarity between two sentences using lemmas and synonyms techniques\n","  @param senteces: tuple with two sentences\n","  @return: similarity between the two sentences using lemmas and synonyms techniques\n","  \"\"\"\n","  sentences = [remove_special_chars(s) for s in sentences]\n","  tokens = [tokenize(s) for s in sentences]\n","  lemmas = [list(map(lambda x: x.lemma_.lower(), t)) for t in tokens]\n","\n","  lemmas[0], lemmas[1] = match_synonyms(lemmas[0], lemmas[1])\n","\n","  return default_sim_function(set(lemmas[0]), set(lemmas[1]))\n","\n","\n","def synset_sim(sentences):\n","  \"\"\"\n","  Obtains the similarity between two sentences using synset technique\n","  @param senteces: tuple with two sentences\n","  @return similarity between the two sentences using synset technique\n","  \"\"\"\n","  sent1, sent2 = sentences\n","  lemmas1, tags1 = get_lemmas(sent1)\n","  lemmas2, tags2 = get_lemmas(sent2)\n","  synsets1 = get_synsets(lemmas1, tags1)\n","  synsets2 = get_synsets(lemmas2, tags2)\n","\n","  distance = 1\n","  if synsets1 or synsets2:\n","    distance = default_sim_function(set(synsets1), set(synsets2))\n","  \n","  return distance\n","\n","\n","def spacy_synset_sim(sentences):\n","  \"\"\"\n","  Obtains the similarity between two sentences using Spacy synset technique\n","  @param senteces: tuple with two sentences\n","  @return similarity between the two sentences using Spacy synset technique\n","  \"\"\"\n","  # No special chars removing\n","  docs = [nlp(s) for s in sentences]\n","  # Not stopwords removing\n","  \n","  synsets = []\n","  for sent_idx, sent_doc in enumerate(docs):\n","    # Get lemmas and tags\n","    lemmas = []\n","    tags = []\n","    for token in sent_doc:\n","      lemmas.append((token.lemma_.lower()))\n","      tags.append((token.lemma_.lower(), token.pos_))\n","    # Get synsets\n","    synsets.append(get_synsets(lemmas, tags))\n","  \n","  return default_sim_function(set(synsets[0]), set(synsets[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2S1s4q1SBef0"},"source":["#### Lexical + positional"]},{"cell_type":"code","metadata":{"id":"hapRXRiTBiBb"},"source":["def n_grams_sim(sentences, n):\n","  \"\"\"\n","  Obtains the similarity between two sentences using n-grams technique\n","  @param senteces: tuple with two sentences\n","  @param n: n-grams size\n","  @return similarity between the two sentences using n-grams technique\n","  \"\"\"\n","  sents_n_grams = [ [], [] ]\n","  for i, s in enumerate(sentences):\n","    sents_n_grams[i] += get_n_grams(s, n)\n","  \n","  return default_sim_function(set(sents_n_grams[0]), set(sents_n_grams[1]))\n","\n","\n","def n_lemmas_sim(sentences, n):\n","  \"\"\"\n","  Obtains the similarity between two sentences using n-lemmas technique\n","  @param senteces: tuple with two sentences\n","  @param n: n-lemmas size\n","  @return similarity between the two sentences using n-lemmas technique\n","  \"\"\"\n","  sentences = [remove_special_chars(s) for s in sentences]\n","  tokens = [tokenize(s) for s in sentences]\n","  lemmas = [list(map(lambda x: x.lemma_.lower(), t)) for t in tokens]\n","\n","  n_lemmas_lists = [[], []]\n","  for sent_idx, sent_lemmas in enumerate(lemmas):\n","    ini = 0\n","    end = n \n","    while end < len(sent_lemmas)-1:     \n","      n_lemmas = \"\"\n","      for i in range(ini, end):\n","        n_lemmas += sent_lemmas[i]+\"_\"\n","      n_lemmas_lists[sent_idx].append(n_lemmas)\n","      ini += 1\n","      end += 1\n","  \n","  return default_sim_function(set(n_lemmas_lists[0]), set(n_lemmas_lists[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APdXFCBRjaOu"},"source":["#### Compute base similarities"]},{"cell_type":"code","metadata":{"id":"UnkFIXBzjb11"},"source":["######### Define similarity methods with desired configurations #########\n","sim_methods_dict = {\n","    \"NLTK words\": nltk_words_sim,\n","    \"Spacy words (without preprocessing)\": partial(spacy_words_sim, use_preprocessing=False),\n","    \"Word synonyms\": word_synonyms_sim,\n","    \"NLTK lemmas\": nltk_lemmas_sim,\n","    \"Spacy lemmas\": spacy_lemmas_sim,\n","    \"Lemmas synonyms\": lemmas_synonyms_sim,\n","    \"Synsets\": synset_sim,\n","    \"Ngrams2\": partial(n_grams_sim, n=2),\n","    \"Ngrams3\": partial(n_grams_sim, n=3),\n","    \"Ngrams4\": partial(n_grams_sim, n=4),\n","    \"Ngrams5\": partial(n_grams_sim, n=5),\n","    \"Ngrams6\": partial(n_grams_sim, n=6),\n","    \"Ngrams7\": partial(n_grams_sim, n=7),\n","    \"Nlemmas2\": partial(n_lemmas_sim, n=2)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hMYQpUsjehc"},"source":["######### Functions #########\n","def compute_all_similarities(dataset, sim_methods_dict):\n","  \"\"\"\n","  Compute all similarities between pairs of sentences in the dataset\n","  @param dataset: dataframe caontaining sentence pairs\n","  @param sim_methods_dict: dictionary with similarity methods\n","  @action: compute all similarities between pairs of sentences in the dataset and add them to the dataset in the 'similarity' field\n","  \"\"\"\n","  for method_name, method in sim_methods_dict.items():\n","    compute_similarities(dataset, method, method_name)\n","\n","\n","def compute_similarities(dataset, method, method_name):\n","  \"\"\"\n","  Compute similarity between pairs of sentences in the dataset\n","  @param dataset: dataset to compute the similarity\n","  @param method: similarity method\n","  @param method_name: name of the similarity method\n","  @action: save the similarity matrix in a new column in dataset. \n","  \"\"\"\n","  dataset[method_name] = list(map(method, zip(dataset[0], dataset[1])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5s_Hm8PBjfOV"},"source":["######### Compute #########\n","compute_all_similarities(train_dataset, sim_methods_dict)\n","compute_all_similarities(test_dataset, sim_methods_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3jnr-Ym52WD"},"source":["## Regression"]},{"cell_type":"code","metadata":{"id":"nnEbWtU6-uRh"},"source":["######### Define regression methods with desired configurations #########\n","regr_methods_dict = {\"Linear\": linear_model.LinearRegression,\n","                      \"MLP\": partial(MLPRegressor, hidden_layer_sizes=(200, 100, 50), max_iter=1000, random_state=42),\n","                    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihZxePm0kKAn"},"source":["######### Functions #########\n","def train_regression(train_dataset, regr_model, sim_methods_dict):\n","  \"\"\"\n","  Train the regression model\n","  @param train_dataset: dataset to train the regression model\n","  @param regr_model: regression model\n","  @param sim_methods_dict: dictionary with similarity methods\n","  @action train the regression model\n","  \"\"\"\n","  # Get data\n","  train_data = get_sim_numpy_data(train_dataset, sim_methods_dict)\n","  train_labels = train_dataset[\"gs\"].to_numpy()\n","\n","  # Train\n","  regr_model.fit(train_data, train_labels)\n","\n","\n","def regression_predict(dataset, regr_model, sim_methods_dict, regression_method_name):\n","  \"\"\"\n","  Predict the regression model\n","  @param dataset: dataset to predict the regression model\n","  @param regr_model: regression model\n","  @param sim_methods_dict: dictionary with similarity methods\n","  @param regression_method_name: name of the regression method\n","  @action predict the regression model and add it to the dataset\n","  \"\"\"\n","  data = get_sim_numpy_data(dataset, sim_methods_dict)\n","\n","  predictions = regr_model.predict(data)\n","  predictions = np.tanh(predictions)  # Apply hyperbolic tangent function\n","  predictions = (predictions - predictions.min()) / (predictions.max() - predictions.min()) # Min-Max normalization for more intuitive results\n","\n","  dataset[regression_method_name+\" regression\"] = predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqE4FxRq6sSU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638981979699,"user_tz":-60,"elapsed":3890,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"44fb08f5-6801-4b40-cb63-03e5e85863b1"},"source":["######### Perform regression #########\n","regr_models_dict = {}\n","for regr_method_name, method in regr_methods_dict.items():\n","  print(f\"Performing {regr_method_name} regression...\")\n","\n","  regr_models_dict[regr_method_name] = regr_model = method() # Create model\n","  train_regression(train_dataset, regr_model, sim_methods_dict)\n","  \n","  # If Linear, show relevances and weights\n","  if regr_method_name == \"Linear\":\n","    print(\"Base similarities relevances and weights in linear regression:\")\n","    norm_weights = abs(regr_model.coef_) / np.sum(abs(regr_model.coef_)) * len(regr_model.coef_)\n","    for i, sim_method_name in enumerate(sim_methods_dict.keys()):\n","      print(f\"[{sim_method_name}] = {norm_weights[i]} ({regr_model.coef_[i]})\")\n","\n","  # Predict\n","  regression_predict(train_dataset, regr_model, sim_methods_dict, regr_method_name)\n","  regression_predict(test_dataset, regr_model, sim_methods_dict, regr_method_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing Linear regression...\n","Base similarities relevances and weights in linear regression:\n","[NLTK words] = 1.4959906842709358 (0.5891151401430089)\n","[Spacy words (without preprocessing)] = 2.9200640861527227 (-1.1499095425041275)\n","[Word synonyms] = 2.105794755059739 (0.8292535410031717)\n","[NLTK lemmas] = 1.576660510393672 (-0.6208825945939491)\n","[Spacy lemmas] = 2.8179972547127026 (1.1097160330525337)\n","[Lemmas synonyms] = 0.3108249955700553 (0.12240163842627261)\n","[Synsets] = 0.1059550020730842 (-0.041724655475082556)\n","[Ngrams2] = 0.5800577323739827 (0.2284244119241096)\n","[Ngrams3] = 0.470425449954287 (0.1852516581757447)\n","[Ngrams4] = 0.1488616205535068 (-0.05862110999510209)\n","[Ngrams5] = 0.011718479635620527 (0.004614690348934886)\n","[Ngrams6] = 0.6016542154652748 (-0.23692902047327513)\n","[Ngrams7] = 0.08536966113411375 (0.0336182306559579)\n","[Nlemmas2] = 0.7686255526503017 (-0.30268166434990507)\n","Performing MLP regression...\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wt28L0PD9-W7"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"QnIxWhPL-aMW"},"source":["######### Functions #########\n","def evaluate(dataset):\n","  \"\"\"\n","  Obtain and show the correlation between similarity measures and gold standard for a given dataset and print the results.\n","  @param dataset: Dataframe with similarity measures and gold standard.\n","  @return correlations dictionary for each similarity measure\n","  \"\"\"\n","  correlations = compute_correlations(dataset)\n","  \n","  for method_name, correlation in correlations.items():\n","    print(f\"[ Gold Standard <-> {method_name} ] correlation = {round(correlation,4)}\")\n","  \n","  return correlations\n","\n","\n","def compute_correlations(dataset):\n","  \"\"\"\n","  Compute correlations for a given dataset\n","  @param dataset: Dataframe with similarity measures and gold standard.\n","  @return correlations dictionary for each similarity measure\n","  \"\"\"\n","  correlations = {}\n","  methods_names = list(filter(lambda x: x not in [0, 1, \"gs\"], dataset.columns))\n","  for method_name in methods_names:\n","    correlations[method_name] = pearsonr(dataset[\"gs\"], dataset[method_name])[0]\n","  \n","  return correlations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3LU-jgzITkG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638981979700,"user_tz":-60,"elapsed":5,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"f915ac76-c547-4234-acf9-9b14940d30a1"},"source":["######### Evaluate #########\n","print(\"Train results:\")\n","correlations = evaluate(train_dataset)\n","print(\"Test results:\")\n","correlations = evaluate(test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train results:\n","[ Gold Standard <-> NLTK words ] correlation = 0.4396\n","[ Gold Standard <-> Spacy words (without preprocessing) ] correlation = 0.4167\n","[ Gold Standard <-> Word synonyms ] correlation = 0.5526\n","[ Gold Standard <-> NLTK lemmas ] correlation = 0.351\n","[ Gold Standard <-> Spacy lemmas ] correlation = 0.6356\n","[ Gold Standard <-> Lemmas synonyms ] correlation = 0.5714\n","[ Gold Standard <-> Synsets ] correlation = 0.3929\n","[ Gold Standard <-> Ngrams2 ] correlation = 0.3078\n","[ Gold Standard <-> Ngrams3 ] correlation = 0.1496\n","[ Gold Standard <-> Ngrams4 ] correlation = 0.1161\n","[ Gold Standard <-> Ngrams5 ] correlation = 0.0745\n","[ Gold Standard <-> Ngrams6 ] correlation = 0.0569\n","[ Gold Standard <-> Ngrams7 ] correlation = 0.0402\n","[ Gold Standard <-> Nlemmas2 ] correlation = 0.1519\n","[ Gold Standard <-> Linear regression ] correlation = 0.7756\n","[ Gold Standard <-> MLP regression ] correlation = 0.8495\n","Test results:\n","[ Gold Standard <-> NLTK words ] correlation = 0.4203\n","[ Gold Standard <-> Spacy words (without preprocessing) ] correlation = 0.4105\n","[ Gold Standard <-> Word synonyms ] correlation = 0.5236\n","[ Gold Standard <-> NLTK lemmas ] correlation = 0.359\n","[ Gold Standard <-> Spacy lemmas ] correlation = 0.5907\n","[ Gold Standard <-> Lemmas synonyms ] correlation = 0.541\n","[ Gold Standard <-> Synsets ] correlation = 0.4062\n","[ Gold Standard <-> Ngrams2 ] correlation = 0.3073\n","[ Gold Standard <-> Ngrams3 ] correlation = 0.2431\n","[ Gold Standard <-> Ngrams4 ] correlation = 0.2371\n","[ Gold Standard <-> Ngrams5 ] correlation = 0.2063\n","[ Gold Standard <-> Ngrams6 ] correlation = 0.19\n","[ Gold Standard <-> Ngrams7 ] correlation = 0.1497\n","[ Gold Standard <-> Nlemmas2 ] correlation = 0.1756\n","[ Gold Standard <-> Linear regression ] correlation = 0.7054\n","[ Gold Standard <-> MLP regression ] correlation = 0.7614\n"]}]},{"cell_type":"markdown","metadata":{"id":"hEoKqdraX66s"},"source":["## Interactive test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZhuNP5LX91e","executionInfo":{"status":"ok","timestamp":1638981979700,"user_tz":-60,"elapsed":4,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"dec1d142-fb7e-4ca9-d203-14fabf3e329e","cellView":"form"},"source":["# Set sentences\n","sentence1 =   \"cats are jolly\"#@param {type:\"string\"}\n","sentence2 = \"cats are pretty\" #@param {type:\"string\"}\n","sentences = (sentence1, sentence2)\n","\n","# Get fundamental similarities\n","similarities = {}\n","for sim_method_name, sim_method in sim_methods_dict.items():\n","  similarities[sim_method_name] = sim_method(sentences)\n","fund_sims = np.array([list(similarities.values())])\n","\n","# Perform regression\n","for regr_model_name, regr_model in regr_models_dict.items():\n","  pred = regr_model.predict(fund_sims)[0]\n","  pred = max(min(pred, 1), 0) # Limit in range [0, 1]\n","  similarities[regr_model_name] = pred\n","\n","# Print results\n","print(\"Similarities:\")\n","for name, value in similarities.items():\n","  print(f\"[{name}] = {round(value,4)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarities:\n","[NLTK words] = 0.5\n","[Spacy words (without preprocessing)] = 0.5\n","[Word synonyms] = 1.0\n","[NLTK lemmas] = 0.5\n","[Spacy lemmas] = 0.3209\n","[Lemmas synonyms] = 1.0\n","[Synsets] = 1.0\n","[Ngrams2] = 0.5\n","[Ngrams3] = 0.6\n","[Ngrams4] = 0.5\n","[Ngrams5] = 0.3333\n","[Ngrams6] = 0.3333\n","[Ngrams7] = 0.5\n","[Nlemmas2] = 0\n","[Linear] = 1\n","[MLP] = 1\n"]}]}]}