{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab3_Benet_Manzanares_Ramon_Mateo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0uvaSAdmbJ1a"},"source":["\n","#Lab3 : Morphology\n","\n","Authors:<br>\n","* Ram√≥n Mateo Navarro\n","* Benet Manzanares Salor"]},{"cell_type":"markdown","metadata":{"id":"CMlLitkpb9vK"},"source":["##Installation ans imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjTTLIFObAqp","executionInfo":{"status":"ok","timestamp":1633001582025,"user_tz":-120,"elapsed":2949,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"7d496b70-aba7-488e-cad2-79a464202431"},"source":["import os\n","import pandas as pd\n","import nltk\n","\n","from argparse import Namespace\n","from google.colab import drive\n","from scipy.stats import pearsonr\n","from nltk.metrics import jaccard_distance\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"3CCNH4zdcQBa"},"source":["## Settings\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQdS_YcbcTlF","executionInfo":{"status":"ok","timestamp":1633001602789,"user_tz":-120,"elapsed":20779,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"2a7459d3-e38f-43b0-b325-6cbc4b6158d5"},"source":["settings = Namespace()\n","\n","settings.mount_path = \"/content/drive/\"\n","drive.mount(settings.mount_path, force_remount=True)\n","\n","settings.project_folder = \"Benet_MAI/S1/IHLT/IHLT_Labs/Lab3\" #@param {type:\"string\"}\n","settings.project_path = os.path.join(os.path.join(settings.mount_path, \"MyDrive\"), settings.project_folder)\n","settings.input_filename = \"STS.input.SMTeuroparl.txt\" #@param {type:\"string\"}\n","settings.input_filepath = os.path.join(settings.project_path, settings.input_filename)\n","settings.gs_filename = \"STS.gs.SMTeuroparl.txt\" #@param {type:\"string\"}\n","settings.gs_filepath = os.path.join(settings.project_path, settings.gs_filename)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"gIddFXgN7mHi"},"source":["## Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"vSGroB117oDl","executionInfo":{"status":"ok","timestamp":1633001604159,"user_tz":-120,"elapsed":1387,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"c86e937b-546f-478d-d93b-49789ff01501"},"source":["dt = pd.read_csv(settings.input_filepath,sep='\\t', header=None)\n","dt['gs'] = pd.read_csv(settings.gs_filepath, sep='\\t', header=None)\n","dt.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The leaders have now been given a new chance a...</td>\n","      <td>The leaders benefit aujourd' hui of a new luck...</td>\n","      <td>4.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Amendment No 7 proposes certain changes in the...</td>\n","      <td>Amendment No 7 is proposing certain changes in...</td>\n","      <td>5.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Let me remind you that our allies include ferv...</td>\n","      <td>I would like to remind you that among our alli...</td>\n","      <td>4.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The vote will take place today at 5.30 p.m.</td>\n","      <td>The vote will take place at 5.30pm</td>\n","      <td>4.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>5.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0  ...    gs\n","0  The leaders have now been given a new chance a...  ...  4.50\n","1  Amendment No 7 proposes certain changes in the...  ...  5.00\n","2  Let me remind you that our allies include ferv...  ...  4.25\n","3        The vote will take place today at 5.30 p.m.  ...  4.50\n","4  The fishermen are inactive, tired and disappoi...  ...  5.00\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"bIBVMWREB0MK"},"source":["## Experiment"]},{"cell_type":"code","metadata":{"id":"SGor28dncYYH"},"source":["wnl = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijiAcU7aE-qg"},"source":["* Jaccard distance "]},{"cell_type":"code","metadata":{"id":"xBT-gEVLFBnH"},"source":["def words_jaccard_similarity(sentences):\n","  sent1, sent2 = sentences\n","  words1 = set(nltk.word_tokenize(sent1))\n","  words2 = set(nltk.word_tokenize(sent2))\n","  return 1-jaccard_distance(words1, words2)\n","\n","\n","def lemmatize(p):\n","    if p[1][0] in {'N', 'V', 'JJ', 'VB'}:\n","        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n","    return p[0]\n","\n","\n","def get_lemmas(sentence):\n","  words = nltk.word_tokenize(sentence)\n","  tags = nltk.pos_tag(words)\n","  lemmas = [lemmatize(pair) for pair in tags]\n","  return set(lemmas), tags\n","\n","\n","def lemmas_jaccard_similarity(sentences):\n","  sent1, sent2 = sentences\n","  lemmas1, tag1 = get_lemmas(sent1)\n","  lemmas2, tag2 = get_lemmas(sent2)\n","\n","  return 1 - jaccard_distance(lemmas1, lemmas2)\n","  \n","\n","dt[\"words_jaccard\"] = list(map(words_jaccard_similarity, zip(dt[0], dt[1])))\n","dt[\"lemmas_jaccard\"] = list(map(lemmas_jaccard_similarity, zip(dt[0], dt[1])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbsnqCaU3onz"},"source":["# Evaluation of the results\n","In this section we evaluate if, using jaccard similarity, lemmatization offers a better performance than only word tokenizing. To this end, we will show the correlation with the gold standard and the sentences where one approach overcome the other.\n","\n","The following questions will be answered:\n","* **Which is better: words or lemmas?** <br>\n","* **Do you think that could perform better for any pair of texts?** <br>"]},{"cell_type":"markdown","metadata":{"id":"ke1UuoVs3uAI"},"source":["## Which is better: words or lemmas?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDZS0ydTf-vP","executionInfo":{"status":"ok","timestamp":1633001644604,"user_tz":-120,"elapsed":223,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"a2671f67-6ccb-4413-b334-28a5910f75f3"},"source":["gs_words_correlation = pearsonr(dt['gs'], dt['words_jaccard'])[0]\n","gs_lemmas_correlation = pearsonr(dt['gs'], dt['lemmas_jaccard'])[0]\n","\n","print(f\"[ Gold Standard <-> Words Jaccard ] correlation = {gs_words_correlation}\")\n","print(f\"[ Gold Standard <-> Lemmas Jaccard ] correlation = {gs_lemmas_correlation}\")\n","best_approach = \"Lemmatization\" if abs(gs_lemmas_correlation) > abs(gs_words_correlation) else \"Words\"\n","print(f\"{best_approach} offers a better correlation with the Gold Standard\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ Gold Standard <-> Words Jaccard ] correlation = 0.4504977169318684\n","[ Gold Standard <-> Lemmas Jaccard ] correlation = 0.4569107458417673\n","Lemmatization offers a better correlation with the Gold Standard\n"]}]},{"cell_type":"markdown","metadata":{"id":"cmR4OaCRK5Zx"},"source":["* Get sentence pairs where one approach is better than the other and viceversa"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-MviyG77D56","executionInfo":{"status":"ok","timestamp":1633001651384,"user_tz":-120,"elapsed":219,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"f8cca15f-a55d-4326-f383-dc9f7a10461f"},"source":["words_better_than_lemmas_df = dt[abs(dt.gs - dt.words_jaccard * 5) < abs(dt.gs - dt.lemmas_jaccard * 5)]\n","lemmas_better_than_words_df = dt[abs(dt.gs - dt.lemmas_jaccard * 5) < abs(dt.gs - dt.words_jaccard * 5)]\n","  \n","print(f\"#Sentences where the words approach is better = {len(words_better_than_lemmas_df)}\")\n","print(f\"#Sentences where the lemmatization approach is better = {len(lemmas_better_than_words_df)}\")\n","print(f\"#Sentences where both approaches are equal = {len(dt)-(len(words_better_than_lemmas_df)+len(lemmas_better_than_words_df))}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["#Sentences where the words approach is better = 15\n","#Sentences where the lemmatization approach is better = 125\n","#Sentences where both approaches are equal = 319\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xmwm8bbN86Ni"},"source":["* **Which is better: words or lemmas?** <br>\n","As it has a greater correlation with the Gold Standard and there are more sentences pairs where it produces a better similarity, we conclude that the lemmatization approach is better."]},{"cell_type":"markdown","metadata":{"id":"MOOWpSqSGAd8"},"source":["## Do you think that could perform better for any pair of texts?"]},{"cell_type":"markdown","metadata":{"id":"uvlIVXU9C3fp"},"source":["* Showing the sentences where the words approach is better than the lemmatization approach"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wItW0t_J9tF-","executionInfo":{"status":"ok","timestamp":1633001659052,"user_tz":-120,"elapsed":305,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"c8aa60d5-ab41-40a2-8a87-7442b3255c77"},"source":["def print_sentence_pair(row):\n","  print(f\"[ GS = {row['gs']} | Words jaccard = {round(row['words_jaccard'], 3)} | Lemmas jaccard = {round(row['lemmas_jaccard'], 3)} ]\")\n","  print(f\"S1 = {row[0]}\")\n","  print(f\"S2 = {row[1]}\")\n","  lemmas1, tags1 = get_lemmas(row[0])\n","  lemmas2, tags2 = get_lemmas(row[1])\n","  print(f\"Lemmas S1 = {lemmas1}\")\n","  print(f\"Lemmas S2 = {lemmas2}\")\n","  print(f\"Tags S1 = {tags1}\")\n","  print(f\"Tags S2 = {tags2}\\n\")\n","\n","\n","for index, row in words_better_than_lemmas_df.iterrows():\n","  print_sentence_pair(row)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ GS = 5.0 | Words jaccard = 0.833 | Lemmas jaccard = 0.571 ]\n","S1 = Amendment No 7 proposes certain changes in the references to paragraphs.\n","S2 = Amendment No 7 proposes changes to certain paragraphs references.\n","Lemmas S1 = {'paragraph', 'change', 'to', 'propose', 'in', 'the', '.', 'certain', '7', 'amendment', 'reference', 'no'}\n","Lemmas S2 = {'change', 'to', '.', 'certain', '7', 'amendment', 'proposes', 'paragraphs', 'reference', 'no'}\n","Tags S1 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'VBZ'), ('certain', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('paragraphs', 'VB'), ('.', '.')]\n","Tags S2 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'NNS'), ('changes', 'NNS'), ('to', 'TO'), ('certain', 'JJ'), ('paragraphs', 'JJ'), ('references', 'NNS'), ('.', '.')]\n","\n","[ GS = 4.25 | Words jaccard = 0.667 | Lemmas jaccard = 0.429 ]\n","S1 = Maij-Weggen report (A5-0323/2000)\n","S2 = Relation Maij-Weggen (A5-0323/2000)\n","Lemmas S1 = {'a5-0323/2000', '(', 'report', ')', 'Maij-Weggen'}\n","Lemmas S2 = {'a5-0323/2000', '(', 'maij-weggen', ')', 'relation'}\n","Tags S1 = [('Maij-Weggen', 'JJ'), ('report', 'NN'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","Tags S2 = [('Relation', 'NN'), ('Maij-Weggen', 'NNP'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","\n","[ GS = 5.0 | Words jaccard = 0.769 | Lemmas jaccard = 0.643 ]\n","S1 = Amendment No 7 proposes certain changes in the references to paragraphs.\n","S2 = Amendment 7 proposes certain modifications in the references to the paragraphs.\n","Lemmas S1 = {'paragraph', 'change', 'to', 'propose', 'in', 'the', '.', 'certain', '7', 'amendment', 'reference', 'no'}\n","Lemmas S2 = {'paragraph', 'to', 'the', 'in', '.', 'modification', 'certain', '7', 'amendment', 'proposes', 'reference'}\n","Tags S1 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'VBZ'), ('certain', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('paragraphs', 'VB'), ('.', '.')]\n","Tags S2 = [('Amendment', 'NNP'), ('7', 'CD'), ('proposes', 'NNS'), ('certain', 'JJ'), ('modifications', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('paragraphs', 'NN'), ('.', '.')]\n","\n","[ GS = 5.0 | Words jaccard = 0.769 | Lemmas jaccard = 0.643 ]\n","S1 = Amendment No 7 proposes certain changes in the references to paragraphs.\n","S2 = Amendment 7 proposes certain modifications in the references to the paragraphs.\n","Lemmas S1 = {'paragraph', 'change', 'to', 'propose', 'in', 'the', '.', 'certain', '7', 'amendment', 'reference', 'no'}\n","Lemmas S2 = {'paragraph', 'to', 'the', 'in', '.', 'modification', 'certain', '7', 'amendment', 'proposes', 'reference'}\n","Tags S1 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'VBZ'), ('certain', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('paragraphs', 'VB'), ('.', '.')]\n","Tags S2 = [('Amendment', 'NNP'), ('7', 'CD'), ('proposes', 'NNS'), ('certain', 'JJ'), ('modifications', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('paragraphs', 'NN'), ('.', '.')]\n","\n","[ GS = 5.0 | Words jaccard = 0.769 | Lemmas jaccard = 0.643 ]\n","S1 = Amendment No 7 proposes certain changes in the references to paragraphs.\n","S2 = Amendment 7 proposes certain modifications in the references to paragraphs.\n","Lemmas S1 = {'paragraph', 'change', 'to', 'propose', 'in', 'the', '.', 'certain', '7', 'amendment', 'reference', 'no'}\n","Lemmas S2 = {'paragraph', 'to', 'the', 'in', '.', 'modification', 'certain', '7', 'amendment', 'proposes', 'reference'}\n","Tags S1 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'VBZ'), ('certain', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('paragraphs', 'VB'), ('.', '.')]\n","Tags S2 = [('Amendment', 'NNP'), ('7', 'CD'), ('proposes', 'NNS'), ('certain', 'JJ'), ('modifications', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('paragraphs', 'VB'), ('.', '.')]\n","\n","[ GS = 3.5 | Words jaccard = 0.222 | Lemmas jaccard = 0.158 ]\n","S1 = Unfortunately, others separate on the basis of accumulated hatred.\n","S2 = D 'other separate themselves unfortunately because of an accumulated grudge.\n","Lemmas S1 = {'the', 'separate', 'on', 'hatred', '.', 'accumulate', 'Unfortunately', 'of', 'others', ',', 'basis'}\n","Lemmas S2 = {\"'other\", 'separate', 'because', '.', 'd', 'an', 'of', 'unfortunately', 'themselves', 'accumulated', 'grudge'}\n","Tags S1 = [('Unfortunately', 'RB'), (',', ','), ('others', 'NNS'), ('separate', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('accumulated', 'VBN'), ('hatred', 'NN'), ('.', '.')]\n","Tags S2 = [('D', 'NNP'), (\"'other\", 'POS'), ('separate', 'JJ'), ('themselves', 'PRP'), ('unfortunately', 'RB'), ('because', 'IN'), ('of', 'IN'), ('an', 'DT'), ('accumulated', 'JJ'), ('grudge', 'NN'), ('.', '.')]\n","\n","[ GS = 5.0 | Words jaccard = 1.0 | Lemmas jaccard = 0.636 ]\n","S1 = The fishermen are inactive, tired and disappointed.\n","S2 = The fishermen inactive, are tired and disappointed.\n","Lemmas S1 = {'The', 'and', 'disappointed', '.', 'be', 'inactive', ',', 'fisherman', 'tired'}\n","Lemmas S2 = {'The', 'and', '.', 'tire', 'be', 'inactive', 'disappoint', ',', 'fisherman'}\n","Tags S1 = [('The', 'DT'), ('fishermen', 'NNS'), ('are', 'VBP'), ('inactive', 'JJ'), (',', ','), ('tired', 'JJ'), ('and', 'CC'), ('disappointed', 'JJ'), ('.', '.')]\n","Tags S2 = [('The', 'DT'), ('fishermen', 'NNS'), ('inactive', 'JJ'), (',', ','), ('are', 'VBP'), ('tired', 'VBN'), ('and', 'CC'), ('disappointed', 'VBN'), ('.', '.')]\n","\n","[ GS = 5.0 | Words jaccard = 0.375 | Lemmas jaccard = 0.222 ]\n","S1 = Maij-Weggen report (A5-0323/2000)\n","S2 = The report Maij-Weggen, A5-0323/2000 --\n","Lemmas S1 = {'a5-0323/2000', '(', 'report', ')', 'Maij-Weggen'}\n","Lemmas S2 = {'a5-0323/2000', 'The', 'maij-weggen', 'report', ',', '--'}\n","Tags S1 = [('Maij-Weggen', 'JJ'), ('report', 'NN'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","Tags S2 = [('The', 'DT'), ('report', 'NN'), ('Maij-Weggen', 'NNP'), (',', ','), ('A5-0323/2000', 'NNP'), ('--', ':')]\n","\n","[ GS = 4.5 | Words jaccard = 0.25 | Lemmas jaccard = 0.176 ]\n","S1 = Unfortunately, others separate on the basis of accumulated hatred.\n","S2 = Other separate unfortunately because of an accumulated resentment.\n","Lemmas S1 = {'the', 'separate', 'on', 'hatred', '.', 'accumulate', 'Unfortunately', 'of', 'others', ',', 'basis'}\n","Lemmas S2 = {'separate', 'because', '.', 'Other', 'an', 'of', 'unfortunately', 'accumulated', 'resentment'}\n","Tags S1 = [('Unfortunately', 'RB'), (',', ','), ('others', 'NNS'), ('separate', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('accumulated', 'VBN'), ('hatred', 'NN'), ('.', '.')]\n","Tags S2 = [('Other', 'JJ'), ('separate', 'JJ'), ('unfortunately', 'RB'), ('because', 'IN'), ('of', 'IN'), ('an', 'DT'), ('accumulated', 'JJ'), ('resentment', 'NN'), ('.', '.')]\n","\n","[ GS = 3.75 | Words jaccard = 0.111 | Lemmas jaccard = 0.053 ]\n","S1 = Unfortunately, others separate on the basis of accumulated hatred.\n","S2 = Other diverge unfortunately owing to an accumulated vengeance.\n","Lemmas S1 = {'the', 'separate', 'on', 'hatred', '.', 'accumulate', 'Unfortunately', 'of', 'others', ',', 'basis'}\n","Lemmas S2 = {'to', 'vengeance', '.', 'Other', 'owe', 'an', 'unfortunately', 'accumulated', 'diverge'}\n","Tags S1 = [('Unfortunately', 'RB'), (',', ','), ('others', 'NNS'), ('separate', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('accumulated', 'VBN'), ('hatred', 'NN'), ('.', '.')]\n","Tags S2 = [('Other', 'JJ'), ('diverge', 'NN'), ('unfortunately', 'RB'), ('owing', 'VBG'), ('to', 'TO'), ('an', 'DT'), ('accumulated', 'JJ'), ('vengeance', 'NN'), ('.', '.')]\n","\n","[ GS = 4.75 | Words jaccard = 0.444 | Lemmas jaccard = 0.3 ]\n","S1 = Maij-Weggen report (A5-0323/2000)\n","S2 = Mrs Maij-Weggen report (A5-0323 / 2000)\n","Lemmas S1 = {'a5-0323/2000', '(', 'report', ')', 'Maij-Weggen'}\n","Lemmas S2 = {'(', 'maij-weggen', 'report', ')', 'mr', '2000', 'a5-0323', '/'}\n","Tags S1 = [('Maij-Weggen', 'JJ'), ('report', 'NN'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","Tags S2 = [('Mrs', 'NNP'), ('Maij-Weggen', 'NNP'), ('report', 'NN'), ('(', '('), ('A5-0323', 'NNP'), ('/', 'NNP'), ('2000', 'CD'), (')', ')')]\n","\n","[ GS = 2.75 | Words jaccard = 0.4 | Lemmas jaccard = 0.273 ]\n","S1 = Maij-Weggen report (A5-0323/2000)\n","S2 = Mrs Maij-Weggen's report (5 / 2000)\n","Lemmas S1 = {'a5-0323/2000', '(', 'report', ')', 'Maij-Weggen'}\n","Lemmas S2 = {\"'s\", '(', 'maij-weggen', 'report', ')', 'mr', '/', '2000', '5'}\n","Tags S1 = [('Maij-Weggen', 'JJ'), ('report', 'NN'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","Tags S2 = [('Mrs', 'NNP'), ('Maij-Weggen', 'NNP'), (\"'s\", 'POS'), ('report', 'NN'), ('(', '('), ('5', 'CD'), ('/', 'RB'), ('2000', 'CD'), (')', ')')]\n","\n","[ GS = 4.0 | Words jaccard = 0.75 | Lemmas jaccard = 1.0 ]\n","S1 = (Parliament adopted the legislative resolution)\n","S2 = (the Parliament adopts the legislative resolution)\n","Lemmas S1 = {'resolution', 'the', 'parliament', '(', ')', 'adopt', 'legislative'}\n","Lemmas S2 = {'resolution', 'the', 'parliament', '(', ')', 'adopt', 'legislative'}\n","Tags S1 = [('(', '('), ('Parliament', 'NNP'), ('adopted', 'VBD'), ('the', 'DT'), ('legislative', 'JJ'), ('resolution', 'NN'), (')', ')')]\n","Tags S2 = [('(', '('), ('the', 'DT'), ('Parliament', 'NNP'), ('adopts', 'VBZ'), ('the', 'DT'), ('legislative', 'JJ'), ('resolution', 'NN'), (')', ')')]\n","\n","[ GS = 4.75 | Words jaccard = 0.667 | Lemmas jaccard = 0.429 ]\n","S1 = Maij-Weggen report (A5-0323/2000)\n","S2 = Report/ratio Maij-Weggen (A5-0323/2000)\n","Lemmas S1 = {'a5-0323/2000', '(', 'report', ')', 'Maij-Weggen'}\n","Lemmas S2 = {'a5-0323/2000', 'report/ratio', '(', 'maij-weggen', ')'}\n","Tags S1 = [('Maij-Weggen', 'JJ'), ('report', 'NN'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","Tags S2 = [('Report/ratio', 'NNP'), ('Maij-Weggen', 'NNP'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","\n","[ GS = 4.5 | Words jaccard = 0.2 | Lemmas jaccard = 0.125 ]\n","S1 = Unfortunately, others separate on the basis of accumulated hatred.\n","S2 = Other separate unfortunately a accumulated resentment.\n","Lemmas S1 = {'the', 'separate', 'on', 'hatred', '.', 'accumulate', 'Unfortunately', 'of', 'others', ',', 'basis'}\n","Lemmas S2 = {'separate', '.', 'Other', 'a', 'unfortunately', 'accumulated', 'resentment'}\n","Tags S1 = [('Unfortunately', 'RB'), (',', ','), ('others', 'NNS'), ('separate', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('accumulated', 'VBN'), ('hatred', 'NN'), ('.', '.')]\n","Tags S2 = [('Other', 'JJ'), ('separate', 'JJ'), ('unfortunately', 'RB'), ('a', 'DT'), ('accumulated', 'JJ'), ('resentment', 'NN'), ('.', '.')]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"A-_vpTLWFPYb"},"source":["* Examples of lemmatization-based similairty fails"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8XzZlKx9znj","executionInfo":{"status":"ok","timestamp":1633001668402,"user_tz":-120,"elapsed":226,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"de73a949-223c-4591-bbec-bfab1f7d186b"},"source":["print_sentence_pair(words_better_than_lemmas_df.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ GS = 5.0 | Words jaccard = 0.833 | Lemmas jaccard = 0.571 ]\n","S1 = Amendment No 7 proposes certain changes in the references to paragraphs.\n","S2 = Amendment No 7 proposes changes to certain paragraphs references.\n","Lemmas S1 = {'paragraph', 'change', 'to', 'propose', 'in', 'the', '.', 'certain', '7', 'amendment', 'reference', 'no'}\n","Lemmas S2 = {'change', 'to', '.', 'certain', '7', 'amendment', 'proposes', 'paragraphs', 'reference', 'no'}\n","Tags S1 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'VBZ'), ('certain', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('references', 'NNS'), ('to', 'TO'), ('paragraphs', 'VB'), ('.', '.')]\n","Tags S2 = [('Amendment', 'NNP'), ('No', 'NNP'), ('7', 'CD'), ('proposes', 'NNS'), ('changes', 'NNS'), ('to', 'TO'), ('certain', 'JJ'), ('paragraphs', 'JJ'), ('references', 'NNS'), ('.', '.')]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"8Not8kWWFYh2"},"source":["At the previous sentences pair, the lemmatization problem is caused by the word \"paragraphs\". <br>\n","At the first sentence the word is classified as VB(verb), so it is lemmatizated as \"paragraph\". <br>\n","On the other hand, at the second sentence it is classified as JJ(adjective), so is lemmatizated as \"paragraphs\". <br>\n","Subsequently, the jaccard distance is lower than the for the words approach, where both \"paragraphs\" instances are considered equals.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bl026TmzFjAp","executionInfo":{"status":"ok","timestamp":1632242556725,"user_tz":-120,"elapsed":2,"user":{"displayName":"Benet Manzanares Salor","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12041278320216496290"}},"outputId":"e0cd5ca4-74bf-4624-fb02-8528fa0d243f"},"source":["print_sentence_pair(words_better_than_lemmas_df.iloc[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ GS = 4.25 | Words jaccard = 0.667 | Lemmas jaccard = 0.429 ]\n","S1 = Maij-Weggen report (A5-0323/2000)\n","S2 = Relation Maij-Weggen (A5-0323/2000)\n","Lemmas S1 = {'report', 'a5-0323/2000', ')', 'Maij-Weggen', '('}\n","Lemmas S2 = {'maij-weggen', 'a5-0323/2000', ')', 'relation', '('}\n","Tags S1 = [('Maij-Weggen', 'JJ'), ('report', 'NN'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","Tags S2 = [('Relation', 'NN'), ('Maij-Weggen', 'NNP'), ('(', '('), ('A5-0323/2000', 'NNP'), (')', ')')]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"TSj1FzOvJr3e"},"source":["Equivalently to the first example, the lemmatization problem is caused by the word \"Maij-Weggen\". <br>\n","At the first sentence the word is classified as JJ(adjective), so is lemmatizated as \"Maij-Weggen\". <br>\n","On the other hand, at the second sentence it is classified as NNP(proper noun), so it is incorrectly lemmatizated as \"maij-weggen\" (lower case). <br>\n","Subsequently, the jaccard distance is lower than the for the words approach, where both \"Maij-Weggen\" instances are considered equals."]},{"cell_type":"markdown","metadata":{"id":"okkIVB2c9qg4"},"source":["* **Do you think that could perform better for any pair of texts?** <br>\n","As has been seen, the lemmatization problem for this dataset/corpus is caused by incorrect Part-Of-Speech classifications and consequent lemmatization errors. Nevertheless, this is only a noticeable disadavantadge compared to only tokenizing words if sentence similarity is commonly related to high vocabulary sharing (many identical words). Consequently, for other datasets/corpora with equal or less vocabulary sharing (less identical words between sentence pairs), lemmatization should obtain a better performance."]}]}